
# ADAS Alert Data Preprocessing
This notebook outlines the data preprocessing steps for the ADAS Alert Analysis project. We will explore the data, clean it, and prepare it for further analysis. Each section includes theoretical explanations, code snippets, and visual outputs.

## 1. Introduction to the Dataset
Before diving into the preprocessing steps, let's first understand the structure of the dataset. The data consists of approximately 6700 rows, each representing an alert generated by an Advanced Driver Assistance System (ADAS) across different vehicles.

### *Dataset Overview*
**Alert:** Type of ADAS alert generated.
**Date:** Date when the alert was triggered.
**Time:** Exact time the alert was generated.
**Lat:** Latitude of the vehicle's location.
**Long:** Longitude of the vehicle's location.
**Vehicle:** Identifier for the vehicle that generated the alert.
**Speed:** The speed of the vehicle at the time of alert generation.
## 2. Loading the Dataset
First, we load the dataset using pandas to begin our preprocessing.


    import pandas as pd

    # Load the dataset
    file_path = 'path_to_your_csv_file.csv'  # Update this with the actual path to your dataset
    data = pd.read_csv(file_path)

### Display the first few rows of the dataset
    data.head()
#### 3. Basic Dataset Information
Understanding the basic structure of the dataset is crucial. We will inspect the data types, check for missing values, and get a general overview.

### Basic Information about the dataset
    data.info()
    Handling Missing Values
    Missing data can skew the analysis. We will check for missing values and handle them appropriately.

### Check for any missing values in the dataset
    data.isnull().sum()
## 4. Exploring Unique Vehicles and Alerts
Understanding the variety of vehicles and alerts present in the dataset helps in grasping the breadth of the data.

Identifying Unique Vehicles

### Get the list of unique vehicles in the dataset
    unique_vehicles = data['Vehicle'].unique()
    print(f"Number of unique vehicles: {len(unique_vehicles)}")
    Identifying Unique Alerts

### Get the list of unique alerts in the dataset
    unique_alerts = data['Alert'].unique()
    print(f"Number of unique alert types: {len(unique_alerts)}")
## 5. Date and Time Conversion
For better analysis, the Date and Time columns need to be in the appropriate format. We will convert them to datetime and extract useful components like year, month, and day.

### Convert the 'Date' and 'Time' columns to datetime format
    data['Date'] = pd.to_datetime(data['Date'])
    data['Time'] = pd.to_datetime(data['Time'], format='%H:%M:%S').dt.time

### Extract year, month, and day
    data['Year'] = data['Date'].dt.year
    data['Month'] = data['Date'].dt.month
    data['Day'] = data['Date'].dt.day
## 6. Removing Duplicate Rows
Duplicate entries can distort analysis results. Weâ€™ll check for duplicates and remove them.

### Checking for and removing duplicate rows
    duplicates = data.duplicated()
    print(f"Number of duplicate rows: {duplicates.sum()}")
    data = data.drop_duplicates()
## 7. Saving the Cleaned Data
Finally, we save the cleaned dataset to a new CSV file for further analysis.



### Save the cleaned data
    cleaned_file_path = 'cleaned_data.csv'  # Update this with the desired path for the cleaned data
    data.to_csv(cleaned_file_path, index=False)

    print("Cleaned data saved to:", cleaned_file_path)
